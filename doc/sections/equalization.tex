% 
% MODULE : equalization
%

\newpage
\section{equalization}
\label{module:equalization}
adaptive equalizers: LMS, RLS, blind...

This section describes the functionality of two digital linear adaptive
equalizers implemented...

\subsection{System Description}
\label{module:equalization:system}
Suppose a known transmitted symbol sequence
$\vec{d} = [ d(0), d(1), \ldots ,d(N-1) ]$
which passes through an unknown channel filter $\vec{h}_n$ of length
$q$.
The received symbol at time $n$ is therefore
%
\begin{equation}
    y(n) = \sum\limits_{k=0}^{q-1}{h_n(k)d(n-k)} + \varphi(n)
\end{equation}
%
where $\varphi(n)$ represents white Gauss noise.
The adaptive linear equalizer attempts to use a finite impulse response (FIR)
filter $\vec{w}$ of length $p$ to estimate the transmitted symbol, using only
the received signal vector $\vec{y}$ and the known data sequence $\vec{d}$,
viz
\[
    \hat{d}(n) = \vec{w}_n^T \vec{y}(n)
\]
where $\vec{y}_n = [ y(n), y(n-1),\ldots, y(n-p+1) ]^T$.
Several methods for estimating $\vec{w}$ are known in the literature, and
typically rely on iteratively adjusting $\vec{w}$ with each input though a
recursion algorithm.
This section provides a very brief overview of two prevalent adaptation
algorithms;
for a more in-depth discussion the interested reader is referred to
\cite{Proakis:2001,Haykin:2002}.

\subsection{{\tt eqlms} (least mean-squares equalizer)}
\label{module:equalization:eqlms}
The least mean-squares (LMS) algorithm adapts the coefficients of the filter
estimate using a steepest descent (gradient) of the instantaneous {\it a priori}
error.
The filter estimate at time $n+1$ follows the following recursion
\begin{equation}
\label{eq:lms:weight_update}
\vec{w}_{n+1} = \vec{w}_{n} - \mu \vec{g}_n
\end{equation}
where $\mu$ is the iterative step size, and
$\vec{g}_n$ the normalized gradient vector, estimated from the error signal
and the coefficients vector at time $n$.

\subsection{{\tt eqrls} (recursive least-squares equalizer)}
\label{module:equalization:eqrls}
The recursive least-squares (RLS) algorithm attempts to minimize the
time-average weighted square error of the filter output, viz
\begin{equation}
c(\vec{w}_n) = \sum\limits_{i=0}^{n}{ \lambda^{i-n} \left| d(i)-\hat{d}(i)\right|^2 }
\end{equation}
where the forgetting factor $0<\lambda\leq 1$ which introduces
exponential weighting into past data, appropriate for time-varying
channels.
The solution to minimizing the cost function $c(\vec{w}_n)$ is achieved by
setting its partial derivatives with respect to $\vec{w}_n$ equal to zero.
The solution at time $n$ involves inverting the weighted cross correlation
matrix for $\vec{y}_n$, a computationally complex task.
This step can be circumvented through the use of a recursive algorithm which
attempts to estimate the inverse using the {\it a priori} error from the
output of the filter.
The update equation is
\begin{equation}
\label{eq:rls:weight_update}
\vec{w}_{n+1} = \vec{w}_n + \Delta_{n}
\end{equation}
where the correction factor $\Delta_{n}$ depends on $\vec{y}_n$ and $\vec{w}_n$,
and involves several $p \times p$ matrix multiplications.
The RLS algorithm provides a solution which converges much faster than the LMS
algorithm, however with a significant increase in computational complexity and
memory requirements.

\subsection{Interface}
\label{module:equalization:interface}
The {\tt eqlms} and {\tt eqrls} have nearly identical interfaces, so we will
leave the discussion to the {\tt eqlms} object here.
Like most objects in \liquid, {\tt eqlms} follows the typical
{\tt create()}, {\tt execute()}, {\tt destroy()} lifecycle.
Training is accomplished either one sample at a time, or in a batch cycle.
If trained one sample at a time, the symbols must be trained in the proper
order, otherwise the algorithm won't converge.
Here is a simple example:
%
\input{listings/eqlms_cccf.example.c.tex}
%
For more detailed examples, see
{\tt examples/eqlms\_cccf\_example.c} and
{\tt examples/eqrls\_cccf\_example.c}.

\subsection{Demonstration}
\label{module:equalization:demo}
The performance of the {\tt eqlms} and {\tt eqrls} equalizers are compared by
generating a channel with an impulse response representing a strong
line-of-sight (LoS) component followed by random echoes.
Each was trained on 512 iterations of a known QPSK-modulated training sequence
with learning rate parameters $\mu=0.999$ and $\lambda=0.999$ for the LMS and
RLS algorithms, respectively.
A small amount of noise was injected after the channel filter to demonstrate
the robustness of the algorithms.
The results of two simulations are shown in
figures~\ref{fig:module:equalization:example1} and
\ref{fig:module:equalization:example2};
the first demonstrating a 10-tap equalizer applied to the response of a 6-tap
channel with an SNR of 40dB, while the second demonstrates a 28-tap equalizer
for a 12-tap channel with an SNR of just 10dB.

The passband power spectral densities (PSD) of the channel and the equalizer
outputs are depicted in figures~\ref{fig:module:equalization:example1:psd} and
\ref{fig:module:equalization:example2:psd}.
Notice that the inter-symbol interference of the channel causes its PSD to
have a non-flat response.
Theoretically, if the inter-symbol interference is completely removed, the
response of both the channel and the equalizer will be completely flat
(neglecting any noise present).
While the PSD of the equalized output is nearly flat in the figures,
it is important to realize that these algorithms minimize a cost function
defined as the square of the {\it a priori} filter output error, and do not
necessarily force the PSD to zero.
The classic zero-forcing equalizer has several drawbacks:
\begin{enumerate}
\item the equalizing filter which would give this response is not
      necessarily realizable; that is, not all channels can be
      perfectly inverted,
\item forcing the frequency response to zero increases the noise
      terms of frequencies where the spectra of the channel
      response is low.  In this regard, the zero-forcing equalizer
      only reduces inter-symbol interference and does not maximize
      the ratio of signal power to both interference {\it and}
      noise power as the LMS and RLS algorithms do.
\end{enumerate}
It is interesting to note that both the LMS and RLS equalizers converge to
nearly the same solution.
In both scenarios, however, the RLS equalizer has a slightly lower error after
training while converging to its error minimum much faster.
The RLS equalizer, however, has a much higher computational complexity.

%-------------------- FIGURE: EQUALIZER EXAMPLE 1 --------------------
\begin{figure}
\centering
\mbox{
  \subfigure[PSD] {
      \includegraphics[trim = 16mm 0mm 18mm 0mm, clip, width=6cm]{figures.gen/equalizer_example1_psd}
      \label{fig:module:equalization:example1:psd}
    } \quad
  \subfigure[constellation] {
      \includegraphics[trim = 16mm 0mm 18mm 0mm, clip, width=6cm]{figures.gen/equalizer_example1_const}
      \label{fig:module:equalization:example1:constellation}
    } \quad
}
\mbox{
  \subfigure[taps] {
      \includegraphics[trim = 23mm 0mm 23mm 0mm, clip, width=6cm]{figures.gen/equalizer_example1_taps}
      \label{fig:module:equalization:example1:taps}
    } \quad
  \subfigure[mean-squared error] {
      \includegraphics[trim = 16mm 0mm 16mm 0mm, clip, width=6cm]{figures.gen/equalizer_example1_mse}
      \label{fig:module:equalization:example1:mse}
    } \quad
}
% trim = left bottom right top
\caption{Scenario 1: 10-tap equalizer for a 6-tap channel with 40dB SNR}
\label{fig:module:equalization:example1}
\end{figure}



%-------------------- FIGURE: EQUALIZER EXAMPLE 2 --------------------
\begin{figure}
\centering
\mbox{
  \subfigure[PSD] {
      \includegraphics[trim = 16mm 0mm 18mm 0mm, clip, width=6cm]{figures.gen/equalizer_example2_psd}
      \label{fig:module:equalization:example2:psd}
    } \quad
  \subfigure[constellation] {
      \includegraphics[trim = 16mm 0mm 18mm 0mm, clip, width=6cm]{figures.gen/equalizer_example2_const}
      \label{fig:module:equalization:example2:constellation}
    } \quad
}
\mbox{
  \subfigure[taps] {
      \includegraphics[trim = 23mm 0mm 23mm 0mm, clip, width=6cm]{figures.gen/equalizer_example2_taps}
      \label{fig:module:equalization:example2:taps}
    } \quad
  \subfigure[mean-squared error] {
      \includegraphics[trim = 16mm 0mm 16mm 0mm, clip, width=6cm]{figures.gen/equalizer_example2_mse}
      \label{fig:module:equalization:example2:mse}
    } \quad
}
% trim = left bottom right top
\caption{Scenario 2: 28-tap equalizer for a 12-tap channel with 10dB SNR}
\label{fig:module:equalization:example2}
\end{figure}


