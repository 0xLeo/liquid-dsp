% 
% MODULE : optim
%

\newpage
\section{optim (optimization)}
\label{module:optim}
Newton-raphson, gradient, evolutionary algorithms, etc.

\subsection{Gradient search}
\label{module:optim:gradient_search}
This module implements a gradient or ``steepest-descent'' search.
Given a function $f$ which operates on a vector
$\vec{x} = [x_0,x_1,\ldots,x_{N-1}]^T$ of $N$ parameters,
the gradient search method seeks to find the optimum $\vec{x}$ which
minimizes $f(\vec{x})$.

\subsubsection{Theory}
The gradient search is an iterative method, and adjusts $\vec{x}$ proportional
to the negative of the gradient of $f$ evaluated at the current location.
The vector $\vec{x}$ is adjusted by
\[
    \Delta \vec{x}[n+1] = -\gamma[n] \nabla f(\vec{x}[n])
\]
where $\gamma[n]$ is the step size and
$\nabla f(\vec{x}[n])$ is the gradient of $f$ at $\vec{x}$, at the $n^{th}$
iteration.
The gradient is a vector field which points to the greatest rate of increase,
and is computed at $\vec{x}$ as
\[
    \nabla f(\vec{x}) = \left(
        \frac{\partial f}{\partial x_0},
        \frac{\partial f}{\partial x_1},
        \ldots,
        \frac{\partial f}{\partial x_{N-1}}
    \right)
\]
In most non-linear optimization problems, $\nabla f(\vec{x})$ is not known,
and must be approximated for each value of $\vec{x}[n]$ using the finite element
method.
The partial derivative of the $k^{th}$ component is estimated by computing the
slope of $f$ when $x_k$ is increased by a small amount $\Delta$ while holding
all other elements of $\vec{x}$ constant.
This process is repeated for all elements in $\vec{x}$ to compute the gradient
vector.
Mathematically, the $k^{th}$ component of the gradient is approximated by
\[
    \frac{\partial f(\vec{x})}{\partial x_k} \approx 
    \frac{f(x_0,\ldots,x_k+\Delta,\ldots,x_{N-1}) - f(\vec{x})}{\Delta}
\]
Once $\nabla f(\vec{x}[n])$ is known, $\Delta\vec{x}[n+1]$ is computed and the
optimizing vector is updated via
\[
    \vec{x}[n+1] = \vec{x}[n] + \Delta\vec{x}[n+1]
\]

\subsubsection{Momentum constant}
When $f(\vec{x})$ is flat (i.e. $\nabla f(\vec{x})\approx \vec{0}$),
convergence will be slow.
This effect can be mitigated by permitting the update vector equation to
retain a small portion of the previous step vector.
The updated vector at time $n+1$ is
\[
    \vec{x}[n+1] = \vec{x}[n] + \Delta\vec{x}[n+1] + \alpha\Delta\vec{x}[n]
\]
where $\Delta\vec{x}[0] = \vec{0}$.
The update...
\[
    \vec{x}[n+1] = 
        %\Delta\vec{x}[n+1] +
        \sum_{k=0}^{n+1}{\alpha^{k}\Delta\vec{x}[n+1-k]}
\]
which is stable only for $0 \le \alpha < 1$.
For flat regions, the gradient vector $\nabla f(\vec{x})$ is approximately a
constant $\Delta\vec{x}$, and $\vec{x}[n]$ therefore becomes a geometric
series converging to $\Delta\vec{x}/(1-\alpha)$.
This accelerates the algorithm across relatively flat regions of $f$.
The momentum constant additionally adds some stability for regions where the
gradient method tends to oscillate, such as steep valleys in $f$.

\subsubsection{Step size adjustment}
In \liquid, the gradient is normalized to unity (orthonormal).
That is $\|\nabla f(\vec{x}[n])\|=1$.
Furthermore, $\gamma$ is slightly reduced each epoch by a multiplier $\mu$
\[
    \gamma[n+1] = \mu \gamma[n]
\]
This helps improve stability and convergence over regions where the algorithm
might oscillate due to steep values of $f$.
%The default value for $\mu$ is 0.99.

\subsubsection{Usage}
Here is a summary of the parameters used in the gradient search algorithm and
their default values:
\begin{itemize}
\item[$\Delta$] : step size in computing the gradient (default $10^{-6}$)
\item[$\gamma$] : step size in updating $\vec{x}[n]$ (default 0.002)
\item[$\alpha$] : momentum constant (default 0.1)
\item[$\mu$]    : iterative $\gamma$ adjustment factor (default 0.99)
\end{itemize}

Here is an example of how the {\tt gradient\_search} is used:
% gradient_search example
\input{listings/gradient_search.example.c.tex}

\subsection{{\tt quasinewton\_search} }
\label{module:optim:quasinewton_search}
% talking points
%   * second-order gradient search
%   * improved speed
%   * higher complexity
The {\tt quasinewton\_search} object in \liquid\ implements the
Quasi-Newton search algorithm which uses the first- and second-order
derivates (gradient vector and Hessian matrix) in its update equation.
Newtonian-based search algorithms approximate the function to be nearly
quadratic near its optimum which requires the second partial derivative
(Hessian matrix) to be computed or estimated at each iteration.
Quasi-Newton methods circumvent this by approximating the Hessian with
successive gradient computations (or estimations) with each step.
The Quasi-Newton method is usually faster than the gradient search due
in part to its second-order (rather than a first-order) Taylor series
expansion about the function of interest,
however its update criteria is significantly more involved.
In particular the step size %$\alpha$
must be sufficiently conditioned
otherwise the algorithm can result in instability.

% TODO : add longer description

%Like any line search algorithm, the process for updating the estimate
%$x^*$ is as follows:%
%\footnote{
%    For the purposes of discussion, it is assumed that the goals is to
%    find $x^*$ which minimizes the function $f$.
%    In order to find its maximum,
%    the search direction, $\vec{p}$, simply needs to be negated.}
%%
%\begin{enumerate}
%\item choose an initial guess of the estimate, $x_0$
%\item compute 
%\end{enumerate}
%
%%The process iterates over...
%The Hessian of a function $f(x_0,x_1,\ldots,x_{n-1})$ is defined as the
%square $n \times n$ matrix
%
%\begin{equation}
%\label{eq:optim:hessian}
%    H(f) =
%    \begin{bmatrix}
%        \frac{\partial^2 f}{x_0^2}  &
%        \frac{\partial^2 f}{x_0x_1} &
%        \cdots &
%        \frac{\partial^2 f}{x_0x_{n-1}} \\
%        %
%        \frac{\partial^2 f}{x_1x_0} &
%        \frac{\partial^2 f}{x_1^2}  &
%        \cdots &
%        \frac{\partial^2 f}{x_1x_{n-1}} \\
%        %
%        \vdots &
%        \vdots &
%        \vdots &
%        \vdots \\
%        %
%        \frac{\partial^2 f}{x_{n-1}x_0} &
%        \frac{\partial^2 f}{x_{n-1}x_1}  &
%        \cdots &
%        \frac{\partial^2 f}{x_{n-1}^2}
%    \end{bmatrix}
%\end{equation}
%%

An example of the {\tt quasinewton\_search} interface is listed below.
Notice that its interface is virtually identical to that of
{\tt gradient\_search}.
% quasinewton_search example
\input{listings/quasinewton_search.example.c.tex}

\subsection{{\tt ga\_search} genetic algorithm search}
\label{module:optim:ga_search}
% talking points
%   * chromosome
%   * evolutionary algorithms (briefly)
%   * interface

\subsubsection{{\tt chromosome}}
\label{module:optim:ga_search:chromosome}



