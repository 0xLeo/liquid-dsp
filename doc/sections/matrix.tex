% 
% MODULE : matrix
%
\section{matrix}
\label{module:matrix}
Matrices in {\it liquid} are represented as just arrays of a single dimension,
and do not rely on special objects for their manipulation.
\input{listings/matrix.example.c.tex}

\subsection{Basic math operations}
\label{module:matrix:math}
add, sub, mul, div, trans(pose), eye

\subsubsection{{\tt matrix\_eye} (identity matrix)}
The {\tt matrix\_eye} method generates the $n \times n$ identity matrix.
\[
    \vec{I}_n = 
    \begin{bmatrix}
        1 & 0 & \cdots & 0 \\
        0 & 1 & \cdots & 0 \\
        %  &   &        &   \\
        \\
        0 & 0 & \cdots & 1 \\
    \end{bmatrix}
\]

\subsection{Elementary math operations}
\label{module:matrix:elementary}
pivot, swaprows

Swap rows...
\begin{verbatim}
x = 
  0.84381998 -2.38303995  1.43060994 -1.66603994
  3.99475002  0.88066000  4.69372988  0.44563001
  7.28072023 -2.06608009  0.67074001  9.80657005
  6.07741022 -3.93098998  1.22826004 -0.42142001

matrixf_swaprows(x,4,4,0,2);
  7.28072023 -2.06608009  0.67074001  9.80657005
  3.99475002  0.88066000  4.69372988  0.44563001
  0.84381998 -2.38303995  1.43060994 -1.66603994
  6.07741022 -3.93098998  1.22826004 -0.42142001
\end{verbatim}

\subsubsection{Pivoting}
[NOTE: terminology for ``pivot'' is different from literature.]
Given an $n \times m$ matrix $\vec{A}$...
\[
    \vec{A} = 
    \begin{bmatrix}
        A_{0,0}     & A_{0,1}   & \cdots  & A_{0,m-1} \\
        A_{1,0}     & A_{1,1}   & \cdots  & A_{1,m-1} \\
        \\
        A_{n-1,0}   & A_{n-1,1} & \cdots  & A_{n-1,m-1}
    \end{bmatrix}
\]
The pivot element must not be zero.
Pivoting $\vec{A}$ around $\vec{A}_{a,b}$ gives
\[
    \vec{B}_{i,j} = \left(
                    \frac{\vec{A}_{i,b}}{\vec{A}_{a,b}}
                    \right)
                    \vec{A}_{a,j} - \vec{A}_{i,j}
                    \forall i \ne a
\]
Row $a$ is left unchanged in $\vec{B}$.
All elements of $\vec{B}$ in column $b$ are zero except for row $a$.
For our example $4 \times 4$ matrix $\vec{x}$, pivoting around
$\vec{x}_{1,2}$ gives:
\begin{verbatim}
matrixf_pivot(x,4,4,1,2);
  0.37374675  2.65145779  0.00000000  1.80186427
  3.99475002  0.88066000  4.69372988  0.44563001
 -6.70986557  2.19192743  0.00000000 -9.74288940
 -5.03205967  4.16144180  0.00000000  0.53803295
\end{verbatim}

\subsection{Complex math operations}
\label{module:matrix:complex}
inv(ert), gjelim (Gauss-Jordan elimination)

\subsubsection{Inverse}
Given an $n \times n$ matrix $\vec{A}$, augment with $\vec{I}_n$:
\[
    \left[\vec{A}|\vec{I}_n\right] = 
    \left[
    \begin{array}{cccc|cccc}
    A_{0,0}     & A_{0,1}   & \cdots  & A_{0,m-1}   & 1 & 0 & \cdots & 0 \\
    A_{1,0}     & A_{1,1}   & \cdots  & A_{1,m-1}   & 0 & 1 & \cdots & 0 \\
                &           &         &             &   &   &        &   \\
    A_{n-1,0}   & A_{n-1,1} & \cdots  & A_{n-1,m-1} & 0 & 0 & \cdots & 1 \\
    \end{array}
    \right]
\]
Perform elementary operations to convert to its row-reduced echelon form.
The resulting matrix has the identity matrix on the left and $\vec{A}^{-1}$ on
its right, viz
\[
    \left[\vec{I}_n|\vec{A}^{-1}\right] = 
    \left[
    \begin{array}{cccc|cccc}
1 & 0 & \cdots & 0 & A^{-1}_{0,0}   & A^{-1}_{0,1}   & \cdots  & A^{-1}_{0,m-1}   \\
0 & 1 & \cdots & 0 & A^{-1}_{1,0}   & A^{-1}_{1,1}   & \cdots  & A^{-1}_{1,m-1}   \\
  &   &        &   &                &                &         &                  \\
0 & 0 & \cdots & 1 & A^{-1}_{n-1,0} & A^{-1}_{n-1,1} & \cdots  & A^{-1}_{n-1,m-1} \\
    \end{array}
    \right]
\]
The {\tt matrix\_inv} method uses Gauss-Jordan elmination (see 
{\tt matrix\_gjelim}) for row reduction and back-substitution.
Pivot elements in $\vec{A}$ with the largest magnitude are chosen to help
stability in floating-point arithmetic.
\begin{verbatim}
matrixf_inv(x,4,4);
 -0.33453920  0.04643385 -0.04868321  0.23879384
 -0.42204019  0.12152659 -0.07431178  0.06774280
  0.35104612  0.15256262  0.04403552 -0.20177667
  0.13544561 -0.01930523  0.11944833 -0.14921521
\end{verbatim}

\subsubsection{Determinant}
The determinant of an $n \times n$ matrix $\vec{A}$...
In {\it liquid}, the determinant is computed by L/U decomposition of $\vec{A}$
using Doolittle's method (see {\tt matrix\_ludecomp\_doolittle}) and then
computing the product of the diagonal elements of $\vec{U}$, viz
\[
    \det\left(\vec{A}\right) =
    \left|\vec{A}\right| =
    \prod_{k=0}^{n-1}{\vec{U}_{k,k}}
\]
This is equivalent to performing L/U decomposition using Crout's method and
then computing the product of the diagonal elements of $\vec{L}$.
\begin{verbatim}
matrixf_det(X,4,4) = 585.40289307
\end{verbatim}

\subsubsection{LU Decomposition, Crout's Method}
Crout's method decomposes a non-singular $n\times n$ matrix $\vec{A}$ into a
product of a lower triangular $n \times n$ matrix $\vec{L}$ and an upper
triangular $n \times n$ matrix $\vec{U}$. %NOTE : discuss permutation matrix P
In fact, $\vec{U}$ is a unit upper triangular matrix (its values along the
diagonal are 1).

\[
    \vec{L}_{i,k} = \vec{A}_{i,k} -
                    \sum_{t=0}^{k-1}{ \vec{L}_{i,t} \vec{U}_{t,k} }
                    \ \forall k \in \{0,n-1\}, i \in \{k,n-1\}
\]

\[
    \vec{U}_{k,j} = \left[
                    \vec{A}_{k,j} -
                    \sum_{t=0}^{k-1}{ \vec{L}_{k,t} \vec{U}_{t,j} }
                    \right] / \vec{L}_{k,k}
                    \ \forall k \in \{0,n-1\}, j \in \{k+1,n-1\}
\]

\begin{verbatim}
matrixf_ludecomp_crout(X,4,4,L,U,P)
L =
  0.84381998  0.00000000  0.00000000  0.00000000
  3.99475002 12.16227055  0.00000000  0.00000000
  7.28072023 18.49547005 -8.51144791  0.00000000
  6.07741022 13.23228073 -6.81350422 -6.70173073
U =
  1.00000000 -2.82410932  1.69539714 -1.97440207
  0.00000000  1.00000000 -0.17093502  0.68514121
  0.00000000  0.00000000  1.00000000 -1.35225296
  0.00000000  0.00000000  0.00000000  1.00000000
\end{verbatim}

Doolittle's method...
\begin{verbatim}
matrixf_ludecomp_doolittle(X,4,4,L,U,P)
L =
  1.00000000  0.00000000  0.00000000  0.00000000
  4.73412609  1.00000000  0.00000000  0.00000000
  8.62828636  1.52072513  1.00000000  0.00000000
  7.20225906  1.08797777  0.80051047  1.00000000
U =
  0.84381998 -2.38303995  1.43060994 -1.66603994
  0.00000000 12.16227150 -2.07895803  8.33287334
  0.00000000  0.00000000 -8.51144791 11.50963116
  0.00000000  0.00000000  0.00000000 -6.70172977
\end{verbatim}

\subsubsection{Gauss-Jordan Elimination}
Gauss-Jordan elimination converts a $n \times m$ matrix into its row-reduced
echelon form using elementary matrix operations (e.g. pivoting).
This can be used to solve a linear system of $n$ equations
$\vec{A}\vec{x} = \vec{b}$ for the unknown vector $\vec{x}$
\[
    \begin{bmatrix}
        A_{0,0}     & A_{0,1}   & \cdots  & A_{0,n-1} \\
        A_{1,0}     & A_{1,1}   & \cdots  & A_{1,n-1} \\
        \\
        A_{n-1,0}   & A_{n-1,1} & \cdots  & A_{n-1,n-1}
    \end{bmatrix}
    \begin{bmatrix}
        x_{0} \\
        x_{1} \\
        \\
        x_{n-1}
    \end{bmatrix}
    =
    \begin{bmatrix}
        b_{0} \\
        b_{1} \\
        \\
        b_{n-1}
    \end{bmatrix}
\]
The solution for $\vec{x}$ is given by inverting $\vec{A}$ and multiplying
by $\vec{b}$, viz
\[
    \vec{x} = \vec{A}^{-1}\vec{b}
\]
This is also equivalent to augmenting $\vec{A}$ with $\vec{b}$ and
converting it to its row-reduced echelon form.
If $\vec{A}$ is non-singular the resulting $n \times n+1$ matrix will hold
$\vec{x}$ in its last column.
The row-reduced echelon form of a matrix is computed in {\it liquid} using the
Gauss-Jordan elimination algorithm, and can be invoked as such:
\begin{verbatim}
Ab =
  0.84381998 -2.38303995  1.43060994 -1.66603994  0.91488999
  3.99475002  0.88066000  4.69372988  0.44563001  0.71789002
  7.28072023 -2.06608009  0.67074001  9.80657005  1.06552994
  6.07741022 -3.93098998  1.22826004 -0.42142001 -0.81707001
matrixf_gjelim(Ab,4,5)
  1.00000000 -0.00000000  0.00000000 -0.00000000 -0.51971692
 -0.00000000  1.00000000  0.00000000  0.00000000 -0.43340963
 -0.00000000 -0.00000000  1.00000000 -0.00000000  0.64247853
  0.00000000 -0.00000000 -0.00000000  0.99999994  0.35925382
\end{verbatim}
Notice that the result contains $\vec{I}_n$ in its first $n$ rows and $n$
columns (to within machine precision).
[NOTE: row permutations (swapping) might have occurred...]

